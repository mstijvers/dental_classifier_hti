{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torch\n",
    "import cv2\n",
    "#for calling one image\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:14:20.131086Z",
     "start_time": "2023-10-12T09:14:20.123971Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home_folder/Documents/Vakken/HTI/DBM140/dental project/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2349, 3, 224, 224]) torch.Size([1251, 3, 224, 224]) torch.Size([1834, 3, 224, 224]) torch.Size([2382, 3, 224, 224]) torch.Size([1296, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Define a transform. Modify this based on your model's needs\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),               # Convert the PIL Image to a PyTorch tensor\n",
    "    transforms.Resize((224, 224)),      # Resize to the size expected by your model (e.g., 224x224 for many pre-trained models)\n",
    "    # transforms.Normalize(mean, std)   # If your model expects normalized data, add this line with the appropriate mean and std\n",
    "])\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(transform(img))\n",
    "            \n",
    "    # Convert list of tensors to a single tensor (batch)\n",
    "    images_tensor = torch.stack(images)\n",
    "    return images_tensor\n",
    "\n",
    "\n",
    "# Load images\n",
    "folder_path = './dentalclassifier/Dentaldata/gingivitis'\n",
    "gingivitus = load_images_from_folder(folder_path)\n",
    "\n",
    "folder_path = './dentalclassifier/Dentaldata/hypodontia'\n",
    "hypodontia = load_images_from_folder(folder_path)\n",
    "\n",
    "folder_path = './dentalclassifier/Dentaldata/caries'\n",
    "caries = load_images_from_folder(folder_path)\n",
    "\n",
    "folder_path = './dentalclassifier/Dentaldata/discoloration'\n",
    "discoloration = load_images_from_folder(folder_path)\n",
    "\n",
    "folder_path = './dentalclassifier/Dentaldata/calculus'\n",
    "calculus = load_images_from_folder(folder_path)\n",
    "\n",
    "folder_path = './dentalclassifier/Dentaldata/healthyteeth'\n",
    "healthyteeth = load_images_from_folder(folder_path)\n",
    "\n",
    "print(gingivitus.shape, hypodontia.shape, discoloration.shape, caries.shape, calculus.shape)  # Expected shape: [num_images, 3, 224, 224] assuming 3-channel (RGB) images resized to 224x224\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:15:18.731691Z",
     "start_time": "2023-10-12T09:15:02.548025Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# combine tensors\n",
    "\n",
    "# Create labels for the two datasets\n",
    "gingivitis_labels = torch.tensor([0] * len(gingivitus))  # Label 0 for gingivitis\n",
    "hypodontia_labels = torch.tensor([1] * len(hypodontia))  # Label 1 for hypodontia\n",
    "discoloration_labels = torch.tensor([2] * len(discoloration))  # Label 2 for discoloration\n",
    "caries_labels = torch.tensor([3] * len(caries))  # Label 3 for discoloration\n",
    "calculus_labels = torch.tensor([4] * len(calculus   ))  # Label 4 for discoloration\n",
    "healthy_labels = torch.tensor([4] * len(healthyteeth   ))  # Label 4 for discoloration\n",
    "\n",
    "# Create a new label for the group: 0 for gingivitis, 1 for hypodontia\n",
    "group_labels = torch.cat((gingivitis_labels, hypodontia_labels, discoloration_labels, caries_labels, calculus_labels, healthy_labels ), dim=0)\n",
    "\n",
    "# Combine the image tensors and labels into a single dataset\n",
    "combined_images = torch.cat((gingivitus, hypodontia, discoloration, caries, calculus, healthyteeth ), dim=0)\n",
    "dataset = TensorDataset(combined_images, group_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:15:22.611842Z",
     "start_time": "2023-10-12T09:15:18.739738Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# create train and test set\n",
    "\n",
    "# Define the sizes for your train, validation, and test sets (e.g., 70%, 15%, 15%)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size = 64  # Adjust as needed\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:15:29.070020Z",
     "start_time": "2023-10-12T09:15:29.039700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# create model \n",
    "import torch.nn as nn\n",
    "from tanden import SimpleCNN\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = 5  # Five classes: gingivitis, hypodontia, discoloration, caries, calculus\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:16:25.089744Z",
     "start_time": "2023-10-12T09:16:24.956413Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Training and Validation Loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate training loss for the epoch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate validation loss and accuracy for the epoch\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have 'train_dataloader' for training and 'val_dataloader' for validation\n",
    "# train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:16:27.246236Z",
     "start_time": "2023-10-12T09:16:27.239915Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Test Loop\n",
    "def test_model(model, test_loader):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate test loss and accuracy\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have 'test_dataloader' for testing\n",
    "# test_model(model, test_dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:16:35.839075Z",
     "start_time": "2023-10-12T09:16:35.835553Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 1.4951 - Val Loss: 1.0235 - Val Acc: 55.01%\n",
      "Epoch [2/10] - Train Loss: 0.9018 - Val Loss: 0.8520 - Val Acc: 62.80%\n",
      "Epoch [3/10] - Train Loss: 0.7131 - Val Loss: 0.7950 - Val Acc: 65.04%\n",
      "Epoch [4/10] - Train Loss: 0.5831 - Val Loss: 0.6928 - Val Acc: 71.04%\n",
      "Epoch [5/10] - Train Loss: 0.4434 - Val Loss: 0.7265 - Val Acc: 71.64%\n",
      "Epoch [6/10] - Train Loss: 0.3406 - Val Loss: 0.6896 - Val Acc: 73.35%\n",
      "Epoch [7/10] - Train Loss: 0.2587 - Val Loss: 0.7737 - Val Acc: 71.97%\n",
      "Epoch [8/10] - Train Loss: 0.2106 - Val Loss: 0.8256 - Val Acc: 72.03%\n",
      "Epoch [9/10] - Train Loss: 0.1693 - Val Loss: 0.9050 - Val Acc: 72.89%\n",
      "Epoch [10/10] - Train Loss: 0.1419 - Val Loss: 0.9643 - Val Acc: 72.89%\n",
      "Test Loss: 0.9365 - Test Accuracy: 69.83%\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the train_dataloader\n",
    "train_model(model, train_dataloader, val_dataloader, criterion, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "# Test the model using the test_dataloader\n",
    "test_model(model, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-12T09:16:37.863051Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Save the trained model checkpoint\n",
    "checkpoint_path = \"dental_classifier.pth\"\n",
    "torch.save(model.state_dict(), checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:55:23.805347Z",
     "start_time": "2023-10-12T09:55:23.586765Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "# Uploading one image at the time and getting confidence interval\n",
    "\n",
    "num_classes = 5  # Assuming 5 classes\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# Load the model's weights (state_dict) from a saved checkpoint file\n",
    "checkpoint_path = \"dental_classifier.pth\"  # Replace with your actual checkpoint file\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define a transformation for preprocessing the single image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add other transformations as needed (e.g., normalization)\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:56:02.770953Z",
     "start_time": "2023-10-12T09:56:02.587341Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: discoloration\n",
      "Confidence Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and preprocess the single image you want to classify\n",
    "image_path = \"/Users/home_folder/Documents/Vakken/HTI/DBM140/dental project/dentalclassifier/static/images/cropped_mouth.jpg\"  # Replace with the path to your image\n",
    "img = Image.open(image_path)\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)  # Add a batch dimension (single image)\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    output = model(img)\n",
    "\n",
    "# Apply softmax to obtain class probabilities\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probabilities = softmax(output)\n",
    "\n",
    "# Interpret the model's prediction and confidence scores\n",
    "class_labels = [\"gingivitis\", \"hypodontia\", \"discoloration\", \"caries\", \"calculus\", \"healthy teeth\"]\n",
    "predicted_class = class_labels[torch.argmax(probabilities)]\n",
    "confidence_score = torch.max(probabilities).item()\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence Score: {confidence_score:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:05:40.534041Z",
     "start_time": "2023-10-12T10:05:40.524325Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import torch\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:14:27.494656Z",
     "start_time": "2023-10-12T10:14:27.483888Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "image_path = \"/Users/home_folder/Documents/Vakken/HTI/DBM140/dental project/dentalclassifier/static/images/cropped_mouth.jpg\"\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image using the same transformations as before\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust the size as needed for your model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Adjust the mean and std\n",
    "])\n",
    "img = transform(img)\n",
    "\n",
    "# Pass the preprocessed image through the model\n",
    "with torch.no_grad():\n",
    "    img = img.unsqueeze(0)  # Add a batch dimension\n",
    "    output = model(img)\n",
    "    \n",
    "def predict_fn(images):\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        output = model(images)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:19:24.252082Z",
     "start_time": "2023-10-12T10:19:24.226974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.3313,  -2.3091, -11.9167,   7.1369,  15.6075]])\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:20:28.456718Z",
     "start_time": "2023-10-12T10:20:28.445294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:18:29.640731Z",
     "start_time": "2023-10-12T10:18:29.632595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only 2D color images are supported",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#img = np.array(img)\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Obtain the LIME explanation\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m explanation \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexplain_instance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclassifier_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhide_color\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Vakken/HTI/DBM140/dental project/venv/lib/python3.9/site-packages/lime/lime_image.py:184\u001B[0m, in \u001B[0;36mLimeImageExplainer.explain_instance\u001B[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001B[0m\n\u001B[1;32m    182\u001B[0m     segments \u001B[38;5;241m=\u001B[39m segmentation_fn(image)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 184\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    186\u001B[0m fudged_image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hide_color \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Vakken/HTI/DBM140/dental project/venv/lib/python3.9/site-packages/lime/lime_image.py:182\u001B[0m, in \u001B[0;36mLimeImageExplainer.explain_instance\u001B[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001B[0m\n\u001B[1;32m    178\u001B[0m     segmentation_fn \u001B[38;5;241m=\u001B[39m SegmentationAlgorithm(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mquickshift\u001B[39m\u001B[38;5;124m'\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m    179\u001B[0m                                             max_dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[1;32m    180\u001B[0m                                             random_seed\u001B[38;5;241m=\u001B[39mrandom_seed)\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 182\u001B[0m     segments \u001B[38;5;241m=\u001B[39m \u001B[43msegmentation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/Documents/Vakken/HTI/DBM140/dental project/venv/lib/python3.9/site-packages/lime/wrappers/scikit_image.py:117\u001B[0m, in \u001B[0;36mSegmentationAlgorithm.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Vakken/HTI/DBM140/dental project/venv/lib/python3.9/site-packages/skimage/_shared/utils.py:270\u001B[0m, in \u001B[0;36mdeprecate_kwarg.__call__.<locals>.fixed_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    267\u001B[0m         kwargs[new_arg] \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(old_arg)\n\u001B[1;32m    269\u001B[0m \u001B[38;5;66;03m# Call the function with the fixed arguments\u001B[39;00m\n\u001B[0;32m--> 270\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Vakken/HTI/DBM140/dental project/venv/lib/python3.9/site-packages/skimage/segmentation/_quickshift.py:74\u001B[0m, in \u001B[0;36mquickshift\u001B[0;34m(image, ratio, kernel_size, max_dist, return_tree, sigma, convert2lab, rng, channel_axis)\u001B[0m\n\u001B[1;32m     71\u001B[0m image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mastype(float_dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m image\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly 2D color images are supported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# move channels to last position as expected by the Cython code\u001B[39;00m\n\u001B[1;32m     77\u001B[0m image \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmoveaxis(image, source\u001B[38;5;241m=\u001B[39mchannel_axis, destination\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: only 2D color images are supported"
     ]
    }
   ],
   "source": [
    "#img = np.array(img)\n",
    "# Obtain the LIME explanation\n",
    "explanation = explainer.explain_instance(img, classifier_fn=predict_fn, top_labels=5, hide_color=0, num_samples=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:20:38.812735Z",
     "start_time": "2023-10-12T10:20:38.742364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(output_image)\n",
    "plt.axis('off')\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Display the LIME explanation\n",
    "explanation_image, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mark_boundaries(explanation_image / 2 + 0.5, mask))\n",
    "plt.axis('off')\n",
    "plt.title('LIME Explanation')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
